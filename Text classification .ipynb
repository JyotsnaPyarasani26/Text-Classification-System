{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48dd6da",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7826b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from \u001b[1mbusiness\u001b[0m directory\n",
      "Loading data from \u001b[1mentertainment\u001b[0m directory\n",
      "Loading data from \u001b[1mpolitics\u001b[0m directory\n",
      "Loading data from \u001b[1msport\u001b[0m directory\n",
      "Loading data from \u001b[1mtech\u001b[0m directory\n",
      "\n",
      "Entire Data is loaded successfully\n",
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(folder_names, root_path):\n",
    "    doc_list = []\n",
    "    tags = folder_names\n",
    "    \n",
    "    for folder in folder_names:\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "        file_names = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "        \n",
    "        for file_path in file_names:\n",
    "            with open(file_path, encoding=\"latin-1\") as f:\n",
    "                lines = f.readlines()\n",
    "                heading = lines[0].strip()  # Stripping the text by spaces and using the first element as the heading\n",
    "                body = ' '.join([l.strip() for l in lines[1:]])\n",
    "                doc_list.append([folder, heading, body])\n",
    "        \n",
    "        print(f\"Loading data from \\033[1m{folder}\\033[0m directory\")\n",
    "    \n",
    "    print(\"\\nEntire Data is loaded successfully\")\n",
    "    return doc_list\n",
    "\n",
    "# Define the folder names corresponding to the different categories\n",
    "folder_names = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "# Define the root path to the 'News Articles' folder\n",
    "root_path = r'C:\\Users\\Administrator\\Desktop\\NLP\\archive\\BBC News Summary\\News Articles'\n",
    "\n",
    "# Call the load_data function to load the dataset\n",
    "dataset = load_data(folder_names, root_path)\n",
    "\n",
    "# Convert the list of lists into a Pandas DataFrame\n",
    "df = pd.DataFrame(dataset, columns=['category', 'heading', 'body'])\n",
    "\n",
    "# Check how many values are in each category\n",
    "tags_values = df['category'].value_counts()\n",
    "print(tags_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decaeb93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>heading</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                            heading  \\\n",
       "0     business  Ad sales boost Time Warner profit   \n",
       "1     business   Dollar gains on Greenspan speech   \n",
       "2     business  Yukos unit buyer faces loan claim   \n",
       "3     business  High fuel prices hit BA's profits   \n",
       "4     business  Pernod takeover talk lifts Domecq   \n",
       "...        ...                                ...   \n",
       "2220      tech   BT program to beat dialler scams   \n",
       "2221      tech    Spam e-mails tempt net shoppers   \n",
       "2222      tech            Be careful how you code   \n",
       "2223      tech    US cyber security chief resigns   \n",
       "2224      tech   Losing yourself in online gaming   \n",
       "\n",
       "                                                   body  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "...                                                 ...  \n",
       "2220   BT is introducing two initiatives to help bea...  \n",
       "2221   Computer users across the world continue to i...  \n",
       "2222   A new European directive could put software w...  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "2224   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91d52d",
   "metadata": {},
   "source": [
    "### ANALYSING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ecb12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category                                             business\n",
      "heading                     Ad sales boost Time Warner profit\n",
      "body         Quarterly profits at US media giant TimeWarne...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[0]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b14c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "heading     0\n",
       "body        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb264d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ba8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea4071c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ac562",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8955167e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category                                  heading  \\\n",
      "0     business  [ad, sale, boost, time, warner, profit]   \n",
      "1     business        [dollar, gain, greenspan, speech]   \n",
      "2     business   [yuko, unit, buyer, face, loan, claim]   \n",
      "3     business     [high, fuel, price, hit, ba, profit]   \n",
      "4     business     [pernod, takeov, talk, lift, domecq]   \n",
      "...        ...                                      ...   \n",
      "2219      tech      [new, consol, promis, big, problem]   \n",
      "2220      tech       [bt, program, beat, dialler, scam]   \n",
      "2222      tech                             [care, code]   \n",
      "2223      tech        [us, cyber, secur, chief, resign]   \n",
      "2224      tech                      [lose, onlin, game]   \n",
      "\n",
      "                                                   body  \n",
      "0     [quarterli, profit, us, media, giant, timewarn...  \n",
      "1     [dollar, hit, highest, level, euro, almost, th...  \n",
      "2     [owner, embattl, russian, oil, giant, yuko, as...  \n",
      "3     [british, airway, blame, high, fuel, price, dr...  \n",
      "4     [share, uk, drink, food, firm, alli, domecq, r...  \n",
      "...                                                 ...  \n",
      "2219  [make, game, futur, consol, requir, graphic, a...  \n",
      "2220  [bt, introduc, two, initi, help, beat, rogu, d...  \n",
      "2222  [new, european, direct, could, put, softwar, w...  \n",
      "2223  [man, make, sure, us, comput, network, safe, s...  \n",
      "2224  [onlin, role, play, game, enthral, flight, rea...  \n",
      "\n",
      "[2127 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Convert to lowercase\n",
    "df['heading'] = df['heading'].str.lower()\n",
    "df['body'] = df['body'].str.lower()\n",
    "\n",
    "# Step 2: Tokenize the strings into individual words\n",
    "df['heading'] = df['heading'].apply(word_tokenize)\n",
    "df['body'] = df['body'].apply(word_tokenize)\n",
    "\n",
    "# Step 3: Remove punctuation and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation_and_stopwords(tokens):\n",
    "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "df['heading'] = df['heading'].apply(remove_punctuation_and_stopwords)\n",
    "df['body'] = df['body'].apply(remove_punctuation_and_stopwords)\n",
    "\n",
    "# Step 4: Apply stemming (you can also use lemmatization if needed)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "df['heading'] = df['heading'].apply(apply_stemming)\n",
    "df['body'] = df['body'].apply(apply_stemming)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d1ad44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>heading</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>[ad, sale, boost, time, warner, profit]</td>\n",
       "      <td>[quarterli, profit, us, media, giant, timewarn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>[dollar, gain, greenspan, speech]</td>\n",
       "      <td>[dollar, hit, highest, level, euro, almost, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>[yuko, unit, buyer, face, loan, claim]</td>\n",
       "      <td>[owner, embattl, russian, oil, giant, yuko, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>[high, fuel, price, hit, ba, profit]</td>\n",
       "      <td>[british, airway, blame, high, fuel, price, dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>[pernod, takeov, talk, lift, domecq]</td>\n",
       "      <td>[share, uk, drink, food, firm, alli, domecq, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tech</td>\n",
       "      <td>[new, consol, promis, big, problem]</td>\n",
       "      <td>[make, game, futur, consol, requir, graphic, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>[bt, program, beat, dialler, scam]</td>\n",
       "      <td>[bt, introduc, two, initi, help, beat, rogu, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>[care, code]</td>\n",
       "      <td>[new, european, direct, could, put, softwar, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>[us, cyber, secur, chief, resign]</td>\n",
       "      <td>[man, make, sure, us, comput, network, safe, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>[lose, onlin, game]</td>\n",
       "      <td>[onlin, role, play, game, enthral, flight, rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2127 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                  heading  \\\n",
       "0     business  [ad, sale, boost, time, warner, profit]   \n",
       "1     business        [dollar, gain, greenspan, speech]   \n",
       "2     business   [yuko, unit, buyer, face, loan, claim]   \n",
       "3     business     [high, fuel, price, hit, ba, profit]   \n",
       "4     business     [pernod, takeov, talk, lift, domecq]   \n",
       "...        ...                                      ...   \n",
       "2219      tech      [new, consol, promis, big, problem]   \n",
       "2220      tech       [bt, program, beat, dialler, scam]   \n",
       "2222      tech                             [care, code]   \n",
       "2223      tech        [us, cyber, secur, chief, resign]   \n",
       "2224      tech                      [lose, onlin, game]   \n",
       "\n",
       "                                                   body  \n",
       "0     [quarterli, profit, us, media, giant, timewarn...  \n",
       "1     [dollar, hit, highest, level, euro, almost, th...  \n",
       "2     [owner, embattl, russian, oil, giant, yuko, as...  \n",
       "3     [british, airway, blame, high, fuel, price, dr...  \n",
       "4     [share, uk, drink, food, firm, alli, domecq, r...  \n",
       "...                                                 ...  \n",
       "2219  [make, game, futur, consol, requir, graphic, a...  \n",
       "2220  [bt, introduc, two, initi, help, beat, rogu, d...  \n",
       "2222  [new, european, direct, could, put, softwar, w...  \n",
       "2223  [man, make, sure, us, comput, network, safe, s...  \n",
       "2224  [onlin, role, play, game, enthral, flight, rea...  \n",
       "\n",
       "[2127 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c781137c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dollar', 'gain', 'greenspan', 'speech']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['heading'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a85db",
   "metadata": {},
   "source": [
    "#### After preprocessing it is in list format so changing into string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9065610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists in 'heading' column back to strings\n",
    "df['heading'] = df['heading'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# Convert lists in 'body' column back to strings\n",
    "df['body'] = df['body'].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88892fc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>heading</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>ad sale boost time warner profit</td>\n",
       "      <td>quarterli profit us media giant timewarn jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>dollar gain greenspan speech</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>yuko unit buyer face loan claim</td>\n",
       "      <td>owner embattl russian oil giant yuko ask buyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>high fuel price hit ba profit</td>\n",
       "      <td>british airway blame high fuel price drop prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>pernod takeov talk lift domecq</td>\n",
       "      <td>share uk drink food firm alli domecq risen spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tech</td>\n",
       "      <td>new consol promis big problem</td>\n",
       "      <td>make game futur consol requir graphic artist m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>bt program beat dialler scam</td>\n",
       "      <td>bt introduc two initi help beat rogu dialler s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>care code</td>\n",
       "      <td>new european direct could put softwar writer r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>us cyber secur chief resign</td>\n",
       "      <td>man make sure us comput network safe secur res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>lose onlin game</td>\n",
       "      <td>onlin role play game enthral flight realiti pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2127 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                           heading  \\\n",
       "0     business  ad sale boost time warner profit   \n",
       "1     business      dollar gain greenspan speech   \n",
       "2     business   yuko unit buyer face loan claim   \n",
       "3     business     high fuel price hit ba profit   \n",
       "4     business    pernod takeov talk lift domecq   \n",
       "...        ...                               ...   \n",
       "2219      tech     new consol promis big problem   \n",
       "2220      tech      bt program beat dialler scam   \n",
       "2222      tech                         care code   \n",
       "2223      tech       us cyber secur chief resign   \n",
       "2224      tech                   lose onlin game   \n",
       "\n",
       "                                                   body  \n",
       "0     quarterli profit us media giant timewarn jump ...  \n",
       "1     dollar hit highest level euro almost three mon...  \n",
       "2     owner embattl russian oil giant yuko ask buyer...  \n",
       "3     british airway blame high fuel price drop prof...  \n",
       "4     share uk drink food firm alli domecq risen spe...  \n",
       "...                                                 ...  \n",
       "2219  make game futur consol requir graphic artist m...  \n",
       "2220  bt introduc two initi help beat rogu dialler s...  \n",
       "2222  new european direct could put softwar writer r...  \n",
       "2223  man make sure us comput network safe secur res...  \n",
       "2224  onlin role play game enthral flight realiti pe...  \n",
       "\n",
       "[2127 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b1f18",
   "metadata": {},
   "source": [
    "### SPLITTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9c519fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:3]\n",
    "y = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82432436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad sale boost time warner profit</td>\n",
       "      <td>quarterli profit us media giant timewarn jump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar gain greenspan speech</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yuko unit buyer face loan claim</td>\n",
       "      <td>owner embattl russian oil giant yuko ask buyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high fuel price hit ba profit</td>\n",
       "      <td>british airway blame high fuel price drop prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pernod takeov talk lift domecq</td>\n",
       "      <td>share uk drink food firm alli domecq risen spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>new consol promis big problem</td>\n",
       "      <td>make game futur consol requir graphic artist m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>bt program beat dialler scam</td>\n",
       "      <td>bt introduc two initi help beat rogu dialler s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>care code</td>\n",
       "      <td>new european direct could put softwar writer r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>us cyber secur chief resign</td>\n",
       "      <td>man make sure us comput network safe secur res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>lose onlin game</td>\n",
       "      <td>onlin role play game enthral flight realiti pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               heading  \\\n",
       "0     ad sale boost time warner profit   \n",
       "1         dollar gain greenspan speech   \n",
       "2      yuko unit buyer face loan claim   \n",
       "3        high fuel price hit ba profit   \n",
       "4       pernod takeov talk lift domecq   \n",
       "...                                ...   \n",
       "2219     new consol promis big problem   \n",
       "2220      bt program beat dialler scam   \n",
       "2222                         care code   \n",
       "2223       us cyber secur chief resign   \n",
       "2224                   lose onlin game   \n",
       "\n",
       "                                                   body  \n",
       "0     quarterli profit us media giant timewarn jump ...  \n",
       "1     dollar hit highest level euro almost three mon...  \n",
       "2     owner embattl russian oil giant yuko ask buyer...  \n",
       "3     british airway blame high fuel price drop prof...  \n",
       "4     share uk drink food firm alli domecq risen spe...  \n",
       "...                                                 ...  \n",
       "2219  make game futur consol requir graphic artist m...  \n",
       "2220  bt introduc two initi help beat rogu dialler s...  \n",
       "2222  new european direct could put softwar writer r...  \n",
       "2223  man make sure us comput network safe secur res...  \n",
       "2224  onlin role play game enthral flight realiti pe...  \n",
       "\n",
       "[2127 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab40c28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       business\n",
       "1       business\n",
       "2       business\n",
       "3       business\n",
       "4       business\n",
       "          ...   \n",
       "2219        tech\n",
       "2220        tech\n",
       "2222        tech\n",
       "2223        tech\n",
       "2224        tech\n",
       "Name: category, Length: 2127, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f76b29",
   "metadata": {},
   "source": [
    "#### we are converting y column which has business ,tech, etc categories to numbers as machine cants understand text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9172d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116bc6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63668806",
   "metadata": {},
   "source": [
    "### SPLITING TRAIN DATA AND TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e0452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16fe748f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b3014",
   "metadata": {},
   "source": [
    "### APPLYING VECTORIZATION TECHNIQUE - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f65c41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a36a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ff362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = cv.fit_transform(X_train['body']).toarray()\n",
    "X_test_bow = cv.transform(X_test['body']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db016c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701, 16765)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9d66f",
   "metadata": {},
   "source": [
    "### CALLING ML ALGORITHM - NAIVE BAYES AND TRAINING THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9da0165",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e446f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb=MultinomialNB()\n",
    "bnb=BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a6f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d42483",
   "metadata": {},
   "source": [
    "### EVALUATING THE MODEL USING ACCURACY AND CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e3d7394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9225352112676056\n",
      "[[79  2  6  0  6]\n",
      " [ 0 68  1  0  4]\n",
      " [ 8  0 87  1  1]\n",
      " [ 0  1  0 90  0]\n",
      " [ 0  1  2  0 69]]\n",
      "0.9220502820933856\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(X_train_bow,y_train)\n",
    "y_pred = gnb.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(precision_score(y_test,y_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6705238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9906103286384976\n",
      "[[91  0  1  0  1]\n",
      " [ 0 72  0  0  1]\n",
      " [ 0  0 97  0  0]\n",
      " [ 0  0  1 90  0]\n",
      " [ 0  0  0  0 72]]\n",
      "0.9905541905541906\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train_bow,y_train)\n",
    "y_pred = mnb.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(precision_score(y_test,y_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8d26487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530516431924883\n",
      "[[91  0  1  0  1]\n",
      " [ 1 71  0  0  1]\n",
      " [ 9  1 87  0  0]\n",
      " [ 0  0  0 91  0]\n",
      " [ 4  2  0  0 66]]\n",
      "0.9570701450113214\n"
     ]
    }
   ],
   "source": [
    "bnb.fit(X_train_bow,y_train)\n",
    "y_pred = bnb.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(precision_score(y_test,y_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c07a26",
   "metadata": {},
   "source": [
    "### CHECKING WITH ANOTHER ML ALGO- RANDOM FOREST AND ITS ACCURACY -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9bb8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9647887323943662"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049eff1",
   "metadata": {},
   "source": [
    "### CHECKING THE ACCURACY WITH N GRAMS VECTORIZATION METHOD  AND RANDOM FOREST ALGO\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7402355f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595505617977528"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),max_features=5000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['body']).toarray()\n",
    "X_test_bow = cv.transform(X_test['body']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c543f",
   "metadata": {},
   "source": [
    "## APPLYING VECTORIZATION TECHNIQUE - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3581c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61b7257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train['body']).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c766b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## applying Guassian naive bayes on tfidf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m gnb\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf,y_train)\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mgnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tfidf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score,confusion_matrix\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(y_test,y_pred))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:105\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:273\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:845\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[0;32m    844\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 845\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:522\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    519\u001b[0m _check_large_sparse(spmatrix, accept_large_sparse)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sparse matrix was passed, but dense \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is required. Use X.toarray() to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accept_sparse, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accept_sparse) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "## applying Guassian naive bayes on tfidf\n",
    "\n",
    "gnb.fit(X_train_tfidf,y_train)\n",
    "y_pred = gnb.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45508d00",
   "metadata": {},
   "source": [
    "### USING TFIDF AND RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "488d0381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952808988764045"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0163305",
   "metadata": {},
   "source": [
    "### USING TFIDF AND NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b9831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a3c5ad",
   "metadata": {},
   "source": [
    "# USING WORD TO VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
